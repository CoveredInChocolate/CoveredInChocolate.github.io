<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="styling.css">
    <script>
        MathJax = {
            loader: {
                load: ['[tex]/mathtools', '[tex]/boldsymbol', '[tex]/color']
            },
            tex: {
                packages: {'[+]': ['mathtools', 'boldsymbol', 'color']},
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    <title>2.1 - Matrix Operations</title> <!-- #################### TITLE #################### -->
</head>
\(
   \def\R{\mathbb{R}}
   \def\N{\mathbb{N}}
   \def\Z{\mathbb{Z}}
   \def\Q{\mathbb{Q}}
   \def\eps{\varepsilon}
   \def\epsilon{\varepsilon}
   \newcommand\bs[1]{\boldsymbol{#1}}
   \renewcommand{\geq}{\geqslant}
   \renewcommand{\leq}{\leqslant}
\)
<body>
    <div class="page">
<h2>Chapter 2 - Matrix Algebra</h2> <!-- Chapter -->

<h1>2.1 - Matrix Operations</h1> <!-- Section -->
<br />
<table border="0">
    <tr>
        <td><b>Main</b>:</td>
        <td><a href="index.html">Index</a></td>
    </tr>
    <tr>
        <td><b>Previous</b>:</td>
        <td><a href="1.9.html">1.9 - The Matrix of a Linear Transformation</a></td>
    </tr>
    <tr>
        <td><b>Next</b>:</td>
        <td><a href="2.2.html">2.2 - The Inverse of a Matrix</a></td>
    </tr>
</table>
<br /><br />

<h2>Results</h2>

Many calculations can be simplified if we do the calculations directly on
the matrix. If A is a m&times;n matrix, the scalar entry in row i and column j of
A is denoted by $a_{ij}$ and is called the (i,j)-entry.

Matrix when highlighting columns:
$$
A =
\big[\bs{a}_1 \;\; \cdots \;\; \bs{a}_j \;\; \cdots \;\; \bs{a}_n \big]
$$
Matrix with the elements:
$$
A =
\begin{bmatrix*}[ccccc]
a_{11} & \cdots & a_{1j} & \cdots & a_{1n} \\ 
\vdots &  & \vdots &  & \vdots \\ 
a_{i1} & \cdots & a_{ij} & \cdots & a_{in} \\ 
\vdots &  & \vdots &  & \vdots \\ 
a_{m1} & \cdots & a_{mj} & \cdots & a_{mn}
\end{bmatrix*}
$$
<br />

The <b>diagonal entries</b> $a_{11}, a_{22}, \ldots$ form the main diagonal.
A diagonal matrix is a square matrix whose nondiagonal elements are nonzero:
$$
\begin{bmatrix*}[rrr]
3 & 0 & 0 \\ 
0 & 2 & 0 \\ 
0 & 0 & 1
\end{bmatrix*}.
$$
An important example of this is the identity matrix, which is a diagonal matrix
where all elements are 1 and are referred to as $I_n$. For example, $I_3$:
$$
\begin{bmatrix*}[rrr]
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1
\end{bmatrix*}.
$$
Finally we have the zero matrix, where all elements are 0. Denoted by <b>0</b> or O:
$$
\begin{bmatrix*}[rrr]
0 & 0 & 0 \\ 
0 & 0 & 0 \\ 
0 & 0 & 0
\end{bmatrix*}.
$$

<h3>Sums and Scalar Multiples</h3>

The vector calculations can be extended to matrices. We say
that two matrcies are <b>equal</b> if they have the same dimension.
When adding two matrices, we simply add each corresponding entry in
the matrix. From this definition, it only applies when the matrices
are equal. We scale a matrix by muliplying with some scalar $r$.
This is then multiplied to every entry of A.

<br /><br />

<div class="thm">Theorem 2.1</div>
<div class="thmtext">
Let A, B and C be matrices of equal size, and let <i>r</i> and <i>s</i>
be some scalars.<br /><br />
&nbsp;&nbsp;<i>(a)</i> $A + B = B + A$<br /><br />
&nbsp;&nbsp;<i>(b)</i> $(A + B) + C = A + (B + C)$<br /><br />
&nbsp;&nbsp;<i>(c)</i> $A + O = A$<br /><br />
&nbsp;&nbsp;<i>(d)</i> $r(A + B) = rA + rB$<br /><br />
&nbsp;&nbsp;<i>(e)</i> $(r + s)A = rA + sA$<br /><br />
&nbsp;&nbsp;<i>(f)</i> $r(sA) = (rs)A$<br />
</div>

<h3>Matrix Multiplication</h3>

When a matrix B is multiplied with a vector <b>x</b>, it is transformed into
another vector B<b>x</b>. If this vector is multiplied by another matrix A,
the result is a third vector A(B<b>x</b>).

<br /><br />
<img src="img/E2.1.1.png" />
<br /><br />

The vector is produced by a <i>composition</i> of mappings. We can represent
this as a single matrix AB.

<br /><br />

If A is an m&times;n matrix, B is an n&times;p matrix, and <b>x</b> is some
vector in ℝ<sup>p</sup>, we denote the columns of B as <b>b</b><sub>j</sub>
and the entries of <b>x</b> by x<sub>j</sub>. Then:
$$
B\bs{x} = x_1\bs{b}_1 + \ldots + x_p\bs{b}_p.
$$
By the linearity of multiplication by A;
$$
\begin{align*}
A(B\bs{x}) &= A\Big(x_1\bs{b}_1 + \ldots + x_p\bs{b}_p\Big)\\
&\\
&= A(x_1\bs{b}_1) + \ldots + A(x_p\bs{b}_p) \\
&\\
&= x_1A\bs{b}_1 + \ldots + x_pA\bs{b}_p
\end{align*}
$$
So, the vector A(B<b>x</b>) is a linear combination of the vectors A<b>b</b><sub>j</sub>
using the entries of <b>x</b> as weights. Expressing as vectors:
$$
A(B\bs{x}) = 
\big[A\bs{b}_1 \;\; \ldots \;\; A\bs{b}_p\big]\bs{x}
$$
Or:
$$
AB = \big[A\bs{b}_1 \;\; \ldots \;\; A\bs{b}_p\big]
$$

<br /><br />
<b>Example</b><br />
<br />
We will look at this in a more concrete setting.
Let A be a 4&times;3 matrix, B a 3&times;2 matrix, and <b>x</b> a
vector in ℝ<sup>2</sup>. Specifically:
$$
A = 
\begin{bmatrix*}[rrr]
3 & 1 & 0 \\ 
1 & 2 & 0 \\ 
1 & 0 & 2 \\ 
2 & 2 & 1
\end{bmatrix*},\qquad
B =
\begin{bmatrix*}[rr]
3 & 1 \\ 
2 & 2 \\ 
1 & 2
\end{bmatrix*},\qquad
\bs{x} =
\begin{bmatrix*}[r]
3 \\ 
2
\end{bmatrix*}
$$
Calculating the two vectors individually. First B<b>x</b>:
$$
\begin{bmatrix*}[rr]
3 & 1 \\ 
2 & 2 \\ 
1 & 2
\end{bmatrix*}
\begin{bmatrix*}[r]
3 \\ 
2
\end{bmatrix*}
=
\begin{bmatrix*}[rcr]
(3)(3) &+& (1)(2) \\ 
(2)(3) &+& (2)(2) \\ 
(1)(3) &+& (2)(2)
\end{bmatrix*}
=
\begin{bmatrix*}[c]
9 + 2 \\ 
6 + 4 \\
3 + 4
\end{bmatrix*}
=
\begin{bmatrix*}[c]
11 \\ 
10 \\
7
\end{bmatrix*}
$$
Now, A(B<b>x</b>):
$$
\begin{bmatrix*}[rrr]
3 & 1 & 0 \\ 
1 & 2 & 0 \\ 
1 & 0 & 2 \\ 
2 & 2 & 1
\end{bmatrix*}
\begin{bmatrix*}[c]
11 \\ 
10 \\
7
\end{bmatrix*}
=
\begin{bmatrix*}[rcrcr]
(3)(11) &+& (1)(10) &+& (0)(7) \\ 
(1)(11) &+& (2)(10) &+& (0)(7) \\ 
(1)(11) &+& (0)(10) &+& (2)(7) \\ 
(2)(11) &+& (2)(10) &+& (1)(7)
\end{bmatrix*}
=
\begin{bmatrix*}[c]
33 + 10 + 0 \\
11 + 20 + 0 \\
11 + 0 + 14 \\
22 + 20 + 7
\end{bmatrix*}
=
\begin{bmatrix*}[c]
43 \\
31 \\
25 \\
49
\end{bmatrix*}
$$
Next, we calculate AB as described. The matrix multiplication is:
(4&times;3)&times;(3&times;2) = 4&times;2. The inner dimensions disappear, so the
resulting matrix has 4 rows and 2 columns. Calculating each column of AB:
$$
A\bs{b}_1 =
\begin{bmatrix*}[rrr]
3 & 1 & 0 \\ 
1 & 2 & 0 \\ 
1 & 0 & 2 \\ 
2 & 2 & 1
\end{bmatrix*}
\begin{bmatrix*}[c]
3 \\ 
2 \\
1
\end{bmatrix*}
=
\begin{bmatrix*}[rcrcr]
(3)(3) &+& (1)(2) &+& (0)(1) \\ 
(1)(3) &+& (2)(2) &+& (0)(1) \\ 
(1)(3) &+& (0)(2) &+& (2)(1) \\ 
(2)(3) &+& (2)(2) &+& (1)(1)
\end{bmatrix*}
=
\begin{bmatrix*}[c]
9 + 2 + 0 \\ 
3 + 4 + 0 \\
3 + 0 + 2 \\
6 + 4 + 1
\end{bmatrix*}
=
\begin{bmatrix*}[c]
11 \\ 
7 \\
5 \\
11
\end{bmatrix*}
$$
$$
A\bs{b}_2 =
\begin{bmatrix*}[rrr]
3 & 1 & 0 \\ 
1 & 2 & 0 \\ 
1 & 0 & 2 \\ 
2 & 2 & 1
\end{bmatrix*}
\begin{bmatrix*}[c]
1 \\ 
2 \\
2
\end{bmatrix*}
=
\begin{bmatrix*}[rcrcr]
(3)(1) &+& (1)(2) &+& (0)(2) \\ 
(1)(1) &+& (2)(2) &+& (0)(2) \\ 
(1)(1) &+& (0)(2) &+& (2)(2) \\ 
(2)(1) &+& (2)(2) &+& (1)(2)
\end{bmatrix*}
=
\begin{bmatrix*}[c]
3 + 2 + 0 \\ 
1 + 4 + 0 \\
1 + 0 + 4 \\
2 + 4 + 2
\end{bmatrix*}
=
\begin{bmatrix*}[c]
5 \\ 
5 \\
5 \\
8
\end{bmatrix*}
$$
Now we have AB:
$$
AB =
\big[A\bs{b}_1 \;\; A\bs{b}_2 \big] =
\begin{bmatrix*}[rr]
11 & 5 \\ 
7 & 5 \\ 
5 & 5 \\ 
11 & 8
\end{bmatrix*}
$$
Verifying:
$$
AB\bs{x} =
\begin{bmatrix*}[rr]
11 & 5 \\ 
7 & 5 \\ 
5 & 5 \\ 
11 & 8
\end{bmatrix*}
\begin{bmatrix*}[r]
3 \\ 
2
\end{bmatrix*}
=
\begin{bmatrix*}[rcr]
(11)(3) &+& (5)(2) \\ 
(7)(3) &+& (5)(2) \\ 
(5)(3) &+& (5)(2) \\ 
(11)(3) &+& (8)(2)
\end{bmatrix*}
=
\begin{bmatrix*}[c]
33 + 10 \\ 
21 + 10 \\
15 + 10 \\
33 + 16
\end{bmatrix*}
=
\begin{bmatrix*}[c]
43 \\ 
31 \\
25 \\
49
\end{bmatrix*}
$$
which is the same as A(B<b>x</b>).

<br /><br />

<div class="thm">Definition</div>
<div class="thmtext">
If A is an m&times;n matrix, and B is an n&times;p matrix with columns <b>b</b><sub>j</sub>,
then the product AB is the m&times;p matrix whose columns are A<b>b</b><sub>j</sub>. That is,
$$
AB =
A\big[\bs{b}_1 \;\; \ldots \;\; \bs{b}_p\big] =
\big[A\bs{b}_1 \;\; \ldots \;\; A\bs{b}_p\big]
$$
</div>
<br />

As we can see, the columns of AB is a liner combintation of the columns of A using
weights from the corresponding column if B.

<br /><br />

<div class="thm">Row-Column Rule for Computing AB</div>
<div class="thmtext">
If the product AB is defined, then the entry in row i and column j of AB
is the sum of the products of corresponding entries from row i of A and
column j of B. If $(AB)_{ij}$ denotes the (i, j)-entry in AB, and if
A is m&times;n matrix, then:
$$
(AB)_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \ldots + a_{in}b_{nj}.
$$
</div>

<br /><br />
<b>Example</b><br />
<br />
Muliplying a 2&times;2 matrix with a 2&times;3 matrix.
Finding the entry in AB positioned in row 1 and column 3. According to the formula above:
$$
(AB)_{13} = a_{11}b_{13} + a_{12}b_{23} 
$$
$$
\begin{bmatrix*}[rr]
\color{red}{2} & \color{red}{3} \\ 
1 & -5
\end{bmatrix*}
\begin{bmatrix*}[rrr]
4 & 3 & \color{blue}{6} \\ 
1 & -2 & \color{blue}{3}
\end{bmatrix*}
=
\begin{bmatrix*}[rrc]
\square& \square& (\color{red}{2}\color{black}{)(}\color{blue}{6}\color{black}{) + (}\color{red}{3}\color{black}{)(}\color{blue}{3}) \\ 
\square& \square& \square
\end{bmatrix*}
=
\begin{bmatrix*}[rrc]
\square& \square& 12 + 9 \\ 
\square& \square& \square
\end{bmatrix*}
=
\begin{bmatrix*}[rrc]
\square& \square& 21 \\ 
\square& \square& \square
\end{bmatrix*}
$$

And for finding row 2 and column 2 of AB:
$$
(AB)_{22} = a_{21}b_{12} + a_{22}b_{22} 
$$
$$
\begin{bmatrix*}[rr]
2 & 3 \\ 
\color{red}{1} & \color{red}{-5}
\end{bmatrix*}
\begin{bmatrix*}[rrr]
4 & \color{blue}{3} & 6\\ 
1 & \color{blue}{-2} & 3
\end{bmatrix*}
=
\begin{bmatrix*}[rcc]
\square& \square& 21 \\ 
\square& (\color{red}{1}\color{black}{)(}\color{blue}{3}\color{black}{) + (}\color{red}{-5}\color{black}{)(}\color{blue}{-2}) & \square
\end{bmatrix*}
=
\begin{bmatrix*}[rcc]
\square& \square& 21 \\ 
\square& 3 + 10& \square
\end{bmatrix*}
=
\begin{bmatrix*}[rcc]
\square& \square& 21 \\ 
\square& 13 & \square
\end{bmatrix*}
$$
The cell in AB determines what <b>row</b> from A we use, and what <b>column</b> in B.

<br /><br />

From the row-column rule, we can make the following observation:
$$
\text{row}_i(AB) = \text{row}_i(A)\cdot B
$$

<h3>Properties of Matrix Multiplication</h3>

<br />

<div class="thm">Theorem 2.2</div>
<div class="thmtext">
Let A be an m&times;n matrix and let B and C have sizes for which the indicated
sums and products are defined. $I_m$ is the m&times;m identity matrix.
<br /><br />
<table>
    <tr>
        <td><i>&nbsp;&nbsp;(a)</i></td>
        <td>&nbsp;$(AB)C = A(BC)$</td>
        <td>&nbsp;&nbsp;&nbsp;</td>
        <td>(Associative law of multiplication)</td>
    </tr><tr><td height="5px"></td></tr>
    <tr>
        <td><i>&nbsp;&nbsp;(b)</i></td>
        <td>&nbsp;$A(B + C) = AB + AC$</td>
        <td>&nbsp;&nbsp;&nbsp;</td>
        <td>(Left distributive law)</td>
    </tr><tr><td height="5px"></td></tr>
    <tr>
        <td><i>&nbsp;&nbsp;(c)</i></td>
        <td>&nbsp;$(B + C)A = BA + CA$</td>
        <td>&nbsp;&nbsp;&nbsp;</td>
        <td>(Right distributive law)</td>
    </tr><tr><td height="5px"></td></tr>
    <tr>
        <td><i>&nbsp;&nbsp;(d)</i></td>
        <td>&nbsp;$r(AB) = (rA)B = A(rB)$</td>
        <td>&nbsp;&nbsp;&nbsp;</td>
        <td>(For any scalar <i>r</i>)</td>
    </tr><tr><td height="5px"></td></tr>
    <tr>
        <td><i>&nbsp;&nbsp;(e)</i></td>
        <td>&nbsp;$I_mA = A = AI_n$</td>
        <td>&nbsp;&nbsp;&nbsp;</td>
        <td>(Identity for matrix multiplication)</td>
    </tr><tr><td height="5px"></td></tr>
</table>
</div>
<br />
Some warnings. The left/right multiplication properties are important. In matrix multiplication
AB is generally not the same as BA. We cannot cancel matrices either. If AB = AC, then it
is not generally true that B = C. And finally, if AB = O, then we can't conclude that
A or B is equal to O.

<h3>Powers of a Matrix</h3>
If A is an n&times;n matrix, and if <i>k</i> is a positive integer, then A<sup>k</sup> denotes
the of <i>k</i> copies of A:
$$
A^k = \underbrace{A\;\cdots\; A}_{k\;\text{times}}
$$

<h3>The Transpose of a Matrix</h3>

Given an m&times;n matrix A, the <b>transpose</b> of A is the n&times;m matrix, denoted by
$A^T$, whose columns are formed from the corresponding rows of A. Examples:
$$
A =
\begin{bmatrix*}[rr]
a & b \\ 
c & d
\end{bmatrix*},\quad
B =
\begin{bmatrix*}[rr]
-5 & 2 \\ 
1 & -3 \\ 
0 & 4
\end{bmatrix*}
,\quad
C =
\begin{bmatrix*}[rrrr]
1 & 1 & 1 & 1 \\ 
-3 & 5 & -2 & 7
\end{bmatrix*}
$$
$$
A^T =
\begin{bmatrix*}[rr]
a & c \\ 
b & d
\end{bmatrix*},\quad
B^T =
\begin{bmatrix*}[rrr]
-5 & 1 & 0 \\ 
2 & -3 & 4 \\ 
\end{bmatrix*}
,\quad
C^T =
\begin{bmatrix*}[rr]
1 & -3 \\ 
1 & 5 \\ 
1 & -2 \\ 
1 & 7 \\ 
\end{bmatrix*}
$$

Informally, the transpose is a matter of "flipping" the matrix around the diagonal.
<br /><br />
<img src="img/Matrix_transpose.gif" />
<br /><br />

<br />

<div class="thm">Theorem 2.3</div>
<div class="thmtext">
Let A and B denote matrices whose size are appropriate for the following
sums and products.
<br />
<br />
<table>
    <tr>
        <td><i>&nbsp;&nbsp;(a)</i></td>
        <td>&nbsp;$(A^T)^T = A$</td>
    </tr><tr><td height="5px"></td></tr>
    <tr>
        <td><i>&nbsp;&nbsp;(b)</i></td>
        <td>&nbsp;$(A + B)^T = A^T + B^T$</td>
    </tr><tr><td height="5px"></td></tr>
    <tr>
        <td><i>&nbsp;&nbsp;(c)</i></td>
        <td>&nbsp;$(rA)^T = rA^T,\qquad \forall r\in\R$</td>
    </tr><tr><td height="5px"></td></tr>
    <tr>
        <td><i>&nbsp;&nbsp;(d)</i></td>
        <td>&nbsp;$(AB)^T = B^TA^T$</td>
    </tr>
</table>
</div>
<br />
The last property carries over to several multiplications. We just reverse the order.
$$
(ABCD)^T = D^TC^TB^TA^T
$$

<br /><br /><br /><br />

<!-- -----------------PARAGRAPH MEDSKIP

<br /><br />


----------------------THM/DEF/RESULT

<br /><br />

<div class="thm">XXX</div>
<div class="thmtext">
asdasd
</div>
<br />

----------------------THM/DEF/RESULT

ℝ<sup>2</sup>
ℝ<sup>3</sup>
ℝ<sup>4</sup>
ℝ<sup>n</sup>
ℝ<sup>m</sup>
ℝ<sup>n</sup>&rarr;ℝ<sup>m</sup>
-->


<!-- ########################### EXERCISES ########################### -->


<!--
<br /><br />
<b>Code</b>:<br />
<div class="rcode">

</div>
<br />
<b>Output</b>:<br />
<pre class="rcode">

</pre>
-->

<h3>Exercise 1</h3>
Perform the given calculations for the following matrices:
$$
A =
\begin{bmatrix*}[rrr]
2 & 0 & -1 \\ 
4 & -5 & 2
\end{bmatrix*},\quad
B = 
\begin{bmatrix*}[rrr]
7 & -5 & 1 \\ 
1 & -4 & -3
\end{bmatrix*},\quad
C = 
\begin{bmatrix*}[rr]
1 & 2 \\ 
-2 & 1
\end{bmatrix*},\quad
D = 
\begin{bmatrix*}[rr]
3 & 5 \\ 
-1 & 4
\end{bmatrix*}
$$
Calculate:
$$
-2A,\quad
B - 2A,\quad
AC,\quad
CD
$$
<br /><br />
<i>Answer</i><br />
-2A. Simply multiplying each element in A with -2.
$$
-2A = 
\begin{bmatrix*}[rrr]
-4 & 0 & 2 \\ 
-8 & 10 & -4
\end{bmatrix*}
$$
B - 2A = B + (-2A)
$$
B + (-2A) = 
\begin{bmatrix*}[rrr]
7 & -5 & 1 \\ 
1 & -4 & -3
\end{bmatrix*}
+
\begin{bmatrix*}[rrr]
-4 & 0 & 2 \\ 
-8 & 10 & -4
\end{bmatrix*}
=
\begin{bmatrix*}[rrr]
3 & -5 & 3 \\ 
-7 & 6 & -7
\end{bmatrix*}
$$
AC. This is not defined since A is a 2&times;3 and C is a 2&times;2 matrix. The
rows of A and columns of C do not match, so the matrix multiplication is not defined.<br />
<br />
CD.
$$
CD = 
\begin{bmatrix*}[rr]
1 & 2 \\ 
-2 & 1
\end{bmatrix*}
\begin{bmatrix*}[rr]
3 & 5 \\ 
-1 & 4
\end{bmatrix*}
=
C[\bs{d}_1\;\bs{d}_2]
=
[C\bs{d}_1\;C\bs{d}_2]
$$
Intermediate calculations:
$$
C\bs{d}_1 =
\begin{bmatrix*}[rr]
1 & 2 \\ 
-2 & 1
\end{bmatrix*}
\begin{bmatrix*}[rr]
3 \\ 
-1
\end{bmatrix*}
=
\begin{bmatrix*}[rcr]
(1)(3) &+& (2)(-1) \\ 
(-2)(3) &+& (1)(-1)
\end{bmatrix*}
=
\begin{bmatrix*}[c]
3 - 2 \\ 
-6 - 1
\end{bmatrix*}
=
\begin{bmatrix*}[r]
1 \\ 
-7
\end{bmatrix*}
$$
$$
C\bs{d}_2 =
\begin{bmatrix*}[rr]
1 & 2 \\ 
-2 & 1
\end{bmatrix*}
\begin{bmatrix*}[rr]
5 \\ 
4
\end{bmatrix*}
=
\begin{bmatrix*}[rcr]
(1)(5) &+& (2)(4) \\ 
(-2)(5) &+& (1)(4)
\end{bmatrix*}
=
\begin{bmatrix*}[c]
5 + 8 \\ 
-10 + 4
\end{bmatrix*}
=
\begin{bmatrix*}[r]
13 \\ 
-6
\end{bmatrix*}
$$
So:
$$
CD = 
\begin{bmatrix*}[rr]
1 & 13 \\ 
-7 & -6
\end{bmatrix*}
$$
<br /><br />
<div class="exend">&#9632;</div>
<br /><br />





<h3>Exercise 5</h3>
Compute the product AB in two ways: (a) by the definition where A<b>b</b><sub>j</sub>
are computed seperately, and (b) by the row-column rule.
$$
A = 
\begin{bmatrix*}[rr]
-1 & 2 \\ 
5 & 4 \\ 
2 & -3
\end{bmatrix*}
,\quad
B = 
\begin{bmatrix*}[rr]
3 & -2 \\ 
-2 & 1
\end{bmatrix*}
$$
<br /><br />
<i>Answer</i><br />
Intermediate calculations:
$$
A\bs{b}_1 =
\begin{bmatrix*}[rr]
-1 & 2 \\ 
5 & 4 \\ 
2 & -3
\end{bmatrix*}
\begin{bmatrix*}[r]
3 \\ 
-2
\end{bmatrix*}
=
\begin{bmatrix*}[rcr]
(-1)(3) &+& (2)(-2) \\ 
(5)(3) &+& (4)(-2) \\ 
(2)(3) &+& (-3)(-2)
\end{bmatrix*}
=
\begin{bmatrix*}[c]
-3 - 4 \\ 
15 -8 \\ 
6 + 6
\end{bmatrix*}
=
\begin{bmatrix*}[r]
-7 \\ 
7 \\ 
12
\end{bmatrix*}
$$
$$
A\bs{b}_2 =
\begin{bmatrix*}[rr]
-1 & 2 \\ 
5 & 4 \\ 
2 & -3
\end{bmatrix*}
\begin{bmatrix*}[r]
-2 \\ 
1
\end{bmatrix*}
=
\begin{bmatrix*}[rcr]
(-1)(-2) &+& (2)(1) \\ 
(5)(-2) &+& (4)(1) \\ 
(2)(-2) &+& (-3)(1)
\end{bmatrix*}
=
\begin{bmatrix*}[r]
2 + 2\\ 
-10 + 4\\
-4 - 3
\end{bmatrix*}
=
\begin{bmatrix*}[r]
4\\ 
-6\\
-7
\end{bmatrix*}
$$
So:
$$
AB =
\begin{bmatrix*}[rr]
-7 &  4\\ 
 7 & -6\\
12 & -7
\end{bmatrix*}
$$
The calculations using the row-column rule will be the same calculations just set
in a single matrix.
<br /><br />
<div class="exend">&#9632;</div>
<br /><br />





<h3>Exercise 10</h3>
Let
$$
A = 
\begin{bmatrix*}[rr]
2 & -3 \\ 
-4 & 6
\end{bmatrix*}
,\quad
B = 
\begin{bmatrix*}[rr]
8 & 4 \\ 
5 & 5
\end{bmatrix*}
,\quad
C = 
\begin{bmatrix*}[rr]
5 & -2 \\ 
3 & 1
\end{bmatrix*},
$$
verify that AB = AC, and yet B&ne;C.
<br /><br />
<i>Answer</i><br />
Skipping all the intermediate calculations.
$$
AB = AC =
\begin{bmatrix*}[rr]
1 & -7 \\ 
-2 & 14
\end{bmatrix*}
$$
<br /><br />
<div class="exend">&#9632;</div>
<br /><br />






<h3>Exercise 15</h3>
Let A, B and C be arbitrary matrices where the indicated sums and products are defined.
Verifying statements. Answer True or False and justify the answer.
<br /><br />
(a)
<i>
    If A and B are 2&times;2 matrices with columns <b>a</b><sub>j</sub>
    and <b>b</b><sub>j</sub>, then
    A = [<b>a</b><sub>1</sub><b>b</b><sub>1</sub>  <b>a</b><sub>2</sub><b>b</b><sub>2</sub>]
</i><br />
Answer: False. Multiplying two columns would produce a scalar, so this result would be a
2&times;1 matrix (vector).
<br /><br />
(b)
<i>
    Each column of AB is a linear combination of the columns of
    B using weights from the corresponding column of A.
</i><br />
Answer: False. The reverse is true:
$$
AB = A[\bs{b}_1 \; \cdots \; \bs{b}_n] = [A\bs{b}_1 \; \cdots \; A\bs{b}_n]
$$
Here we see that the columns of AB is a linear combination of the columns of A, using
the columns of B as weights.
<br /><br />
(c)
<i>
    AB + AC = A(B + C).
</i><br />
Answer: True. This is Theorem 2.2 (b) (switched).
<br /><br />
(d)
<i>
    A<sup>T</sup> + B<sup>T</sup> = (A + B)<sup>T</sup>.
</i><br />
Answer: True. This is Theorem 2.3 (b) (switched).
<br /><br />
(e)
<i>
    The transpose of a product of matrices equals the product
    of their transposes in the same order.
</i><br />
Answer: False. It equals their product in <i>reversed</i> order.
<br /><br />
<div class="exend">&#9632;</div>
<br /><br />



<h3>Exercise 16</h3>
Let A, B and C be arbitrary matrices where the indicated sums and products are defined.
Verifying statements. Answer True or False and justify the answer.
<br /><br />
(a)
<i>
    If A and B are 3&times;3 matrices and
    B = [<b>b</b><sub>1</sub>  <b>b</b><sub>2</sub>  <b>b</b><sub>3</sub>]
    then AB = [A<b>b</b><sub>1</sub>  A<b>b</b><sub>2</sub>  A<b>b</b><sub>3</sub>].
</i><br />
Answer: True. This is basically the definition of matrix multiplication.
<br /><br />
(b)
<i>
    The second row of AB is the second row of A multiplied on the right
    by B.
</i><br />
Answer: True. This is mentioned after the introduction of the row-column rule.
<br /><br />
(c)
<i>
    (AB)C = (AC)B.
</i><br />
Answer: False. This changes the order of the matrix multiplications. The correct statement
would be (AB)C = A(BC).
<br /><br />
(d)
<i>
    (AB)<sup>T</sup> = A<sup>T</sup>B<sup>T</sup>.
</i><br />
Answer: False. The order of matrices should be reversed. As it stands, this operation is
in general not even defined.
<br /><br />
(e)
<i>
    The tranpose of a sum of matrices equals the sum of their transposes.
</i><br />
Answer: True. This is the general statement of Theorem 2.3 (b).
<br /><br />
<div class="exend">&#9632;</div>
<br /><br />





<h3>Exercise 23</h3>
Suppose $CA = I_n$. Show that the equation A<b>x</b> = <b>0</b> only has the trivial
solution. Explain why A cannot have more columns than rows.
<br /><br />
<i>Answer</i><br />
Multiplying with <b>x</b> will give - due to the property
of the identity matrix:
$$
CA\bs{x} = I_n\bs{x} = \bs{x}
$$
Assume now that <b>x</b>&ne;<b>0</b> and that A<b>x</b>=<b>0</b>,
i.e. that A<b>x</b> = <b>0</b> has some nontrivial solution <b>x</b>.
From our equation above, we have shown that:
$$
CA\bs{x} = I_n\bs{x} = \bs{x} \not= \bs{0}.
$$
However, using that A<b>x</b>=<b>0</b> and Theorem 2.2 property (a):
$$
CA\bs{x} = C(A\bs{x}) = C\bs{0} = \bs{0}.
$$
This is a contradiction! Hence, if <b>x</b>&ne;<b>0</b>
then it must follow that A<b>x</b>&ne;<b>0</b>.<br />
<br />
Since A<b>x</b>=<b>0</b> only has the trivial solution, then by
Theorem 1.2 (extended result in section 1.5), there are no free
variables. This means that A has a pivot entry in every column, i.e.
it has n pivot columns. As shown in exercise 40 in section 1.7,
it must then have more rows than columns. 
<br /><br />
<div class="exend">&#9632;</div>
<br /><br />





<h3>Exercise 25</h3>
Suppose A is an m&times;n matrix and there exists n&times;m matrices C and D
such that $CA = I_n$ and $AD = I_m$. Prove that m = n and C = D.<br />
[<i>Hint</i>: Think about the product CAD].
<br /><br />
<i>Answer</i><br />
(Note: this is a reference to the inverted matrix). The statement in
exercise 22 (not done) shows that if B is linearly dependent, then
AB is linearly dependent. The converse of this statement is:
if AB is linearly independent that means that B is linearly independent. In
this exercise, CA = I<sub>n</sub> is linearly independent, and so
A is linearly independent. This means it has pivot entries in each
column, which means we can conclude that m&ge;n.
<br /><br />
By the same general argument, we can conclude that D is linearly independent,
and so we can conclude n&ge;m. Combining these statements, we must
have m = n.
<br /><br />
From this we can determine that $CA = I_n$ and $AD = I_n$. Using the
hint:
$$
\begin{align*}
CA &= I_n \\
CAD &= I_nD \\
CAD &= D \\
C(AD) &= D \\
CI_n &= D \\
C &= D 
\end{align*}
$$
<br /><br />
<div class="exend">&#9632;</div>
<br /><br />



<!--
    ∩ · ≥ ≤
    Ω λ π ≠ ∊ −
    ∅ φ ⇒ ↔	
    ≈ ⊂
-->

<!--

<b>Code</b>:<br />
<div class="rcode">

</div>
<br />
<b>Output</b>:<br />
<pre class="rcode">

</pre>
<br /><br />
-->

<br /><br />
<br /><br />
<br /><br />
<br /><br />
<br /><br />
</div><!-- End page div-->
</body>
</html>

